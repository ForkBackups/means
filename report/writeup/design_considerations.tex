\section{Approach to Software Design}
\label{sec:design_considerations}

Our software development has been heavily inspired by the Agile methodology\cite{_manifesto_????}. 
Agile methodology has been established as an alternative to the traditional waterfall method which views requirements gathering, software design and implementation as a linear process. 
In contrast to this traditional view, agile methodology centres around iterative approach to software development, and aims to minimise the cost caused by problems along the way.

The key principles of this methodology have been neatly summarised in a blog post by Kelly Waters\cite{_what_????}:

\blockquote{
    [The] characteristics that are common to all agile methods, and the things that I think make agile fundamentally different to a more traditional waterfall approach to software development [..] are:
    
    \begin{enumerate}
        \item Active user involvement is imperative 
        \item The team must be empowered to make decisions 
        \item Requirements evolve but the timescale is fixed 
        \item Capture requirements at a high level; lightweight \& visual 
        \item Develop small, incremental releases and iterate 
        \item Focus on frequent delivery of products 
        \item Complete each feature before moving on to the next 
        \item Apply the 80/20 rule 
        \item Testing is integrated throughout the project lifecycle -â€“ test early and often 
        \item A collaborative \& cooperative approach between all stakeholders is essential 
    \end{enumerate}
}

While these may sound very vague requirements, they capture the recommendations based on years of trial-and-error on software development in the industry, and generally act as good guidelines to follow for projects with short deadlines.

\subsection{Requirements Gathering}
One of the key parts of any project is the requirements gathering process -- no amount of skill or hard work will help if the work is misdirected.
Naturally, we took great interest of getting these requirements right.

As per the blogpost cited above, it is suggested to ``capture requirements \emph{at a high level}". 
``High level" is emphasised here to warn about the temptation to get into the discussions about low-level implementation details in the requirements gathering stage. 
These discussions are often in a risk to constrain the requirements with the implementation details, while ideally we would like the requirements to influence the implementation decisions, and not the other way around.

The easiest way to keep the requirements in a high level is to limit the discussion to a set of scenarios of how the users would like to use the software. These scenarios are commonly referred to \emph{user stories} and usually follow the template ``As (some user type, e.g. \emph{a scientist)} I would like to (some action, e.g. \emph{perform these calculations}) because (optionally, the reason for this use case)".
The first part of this relatively free-form stories allows us to get a view of the potential user base of the software, e.g. whether it is researchers who are going to be the software, or maybe it is mostly undergraduate students, each requiring different approach to the problem. The second part of these stories allows us to get a set of important use cases for the problem, while the third part allow us to judge the relative importance of each of the use cases.

We were able to write down four user stories, listed below:
\begin{enumerate}

    \item \emph{As a researcher studying biological systems, I would like to perform a variety of simulations, with different parameter sets, and compare the results of these simulations.}

    \item \emph{As a researcher studying the approximation method, I would like to be able to compare the changes to the approximation quality, when the approximation parameters (i.e. the closure method) are changed.}

    \item \emph{As a researcher studying biological systems, I would like to be able to infer the set of parameters of the model given the data, but this idea is still not very popular in the field and I am not sure if its feasible.}

    \item \emph{As a systems biologist, I have to deal with multiple data formats on case by case basis, and would like to support them all or have a simple format that could be easily writable, because I am computer-savvy enough to write a conversion script.}
\end{enumerate}

The pattern emerging from these use cases is that our project software should not be designed to act as a standalone piece of software, and merely be an intermediate step in some larger data analysis step. For instance, the \emph{comparison} process in the first two user stories could not be clarified in more detail, because the techniques employed in that step are chosen in a very \emph{ad-hoc} fashion. 

Similarly, the fourth user story basically tells us that it is not clear where the input for the model will come from and which format it would be therefore the software needs to be able to either be general enough to work with them all or have a simple enough format for the researcher to do the conversion herself. 
This is not at all surprising, given the exploratory nature of the method, of course, yet not less important to achieve. 

It was this observation that prompted us to move away from the traditional command-line application model for this software. 
In our opinion, flexibility of command line applications comes with the expense of simplicity -- we have all seen really flexible applications that have a list of parameters that is too long for a newcomer to comprehend. Similarly, command line applications create an artificial gap between the data processing and data analysis step, i.e. writing and reading file output, which can be reduced if the application was returning the data in the format that can readily be used to investigate the data.

We believe that the best way we can achieve both flexibility and minimise the gap between the data processing and analysis with the help of package-like structure and interactive python environments, such as IPython notebook\todo{Cite ipython notebook}.
We therefore have spent our development efforts on designing an intuitive modular system for the package, and ensuring integration with this environment. This structure is further described in \autoref{sec:package}.

\subsection{Package Structure}
\label{sec:package}

In order to make the code easier to maintain and manage, we have chosen to structure the main \means  package into nine different smaller submodules, each responsible for a different part of the system. We have taken a great care to ensure that all the necessary features are still importable using the pythonic shorthand \verb"from means import *", as to ensure it is easy enough to use in interactive environments where it might be uncomfortable to deal with modularity explicitly.

The nine submodules mentioned earlier and their functions are listed below:
\begin{enumerate}
    \item \verb"means.core" -- the core functionality, such as the definitions of key classes
    \item \verb"means.approximation" -- implementations of the \gls{lna} and \gls{mea} approximation methods.
    
    \item \verb"means.simulation" -- implementation of various \gls{ode} solving algorithms.
    \item \verb"means.inference" -- implementation of parameter inference methods
    \item \verb"means.io" -- implementation of input and output routines
    \item \verb"means.util" -- utility functions used across the modules, internal to the package
    \item \verb"means.tests" -- various tests for the package functionality, hidden from the user by default
    \item \verb"means.examples" -- definitions of example models that are used throughout tutorials, needs to explicitly be imported by the user.
    \item \verb"means.pipes" -- optional module providing wrappers for pipeline support
\end{enumerate}

Each of these submodules are described in detail below.

\subsubsection{{\tt means.core} -- core classes used within all other packages}
% If you are editing this, please also edit src/means/core/__init__.py docstring
% to keep the two consistent
This package stores the common classes that are used within all of the other means sub-packages.

The module exposes the descriptor classes, such as \verb"Descriptor",
\verb"VarianceTerm", \verb"Moment" and
\verb"ODETermBase" that are used to describe the types of trajectories generated,
as well as certain terms in the ODE equations.

Similarly, both the \verb"StochasticProblem" and
\verb"ODEProblem" classes that are used in stochastic and deterministic simulations respectively are exposed by this module.

Finally, the \verb"Model" class, which provides a standard interface to describe a biological model, and can be thought to be the center of the whole package, is also implemented here.

\subsubsection{{\tt means.approximation} -- approximation methods for biological models}

The approximation submodule implements \gls{ode} and \gls{mea} approximation methods for biological systems encoded by the previously mentioned \verb"Model" objects (see \autoref{sec:making-a-model} for a detailed description on how this encoding is done).

The approximation package is itself split into two sub-packages, one for each of the approximation methods mentioned. The split into sub-packages allows us to keep the routines specific for each of these different methods separate and not polluting each other's namespace. This also provides a clear path for future developers implementing new approximation methods -- just create another sub-package.

Both of the approximation methods, while in a different packages, still inherit from the same base class, intuitively named \verb"ApproximationBaseClass", which defines the common interface all approximation methods need to provide, namely the dependancy on \verb"Model" and public \verb"run()" method that does all the heavy lifting and returns a \verb"ODEProblem" object that can be simulated.
This common interface is required for any pipeline using the approximation methods to be method-agnostic. In other words, we want the pipeline to be able to perform the approximation in exactly the same way, regardless whether \gls{lna} or \gls{mea} is used. 
It is easy to see how this design pattern increases the code maintainability and compatibility as well: not only the future maintainer will have a clear template to follow for implementing the approximation method, but it will also be immediately compatible with all the packages using the approximations.

The advantages described above apply for mostly software that is using all or part of the \means package to perform designated tasks. The necessity of creating approximation objects and running the \verb"run()" function on them to obtain the ordinary differential equations soon becomes a burden in the interactive environments. In order to overcome this, we also provide shorthand functions \verb"mea_approximation" and \verb"lna_approximation", the use of which is demonstrated in \autoref{sec:approximation-example}.

\subsubsection{{\tt means.simulation} -- stochastic and deterministic simulation routines}

The next submodule in the list provides the routines for performing deterministic and stochastic simulations. These simulations are performed with the help of \verb"Simulation" and \verb"SSASimulation" classes defined in this sub-module.

Unlike the approximation package, the simulation routines could not share the same interface, as they have different inputs -- stochastic simulation need \verb"StochasticProblem" that provides the stoichiometry matrix as well as hazard functions to simulate stochastically, while the deterministic simulation routine requires a \verb"ODEProblem" object that is essentially a collection of ordinary differential equations.
Nevertheless, the classes still share the same interface, that provides a \verb"simulate_system()" method which in turn generates a set of \verb"Trajectory" objects, to keep them consistent.

Also unlike the approximation submodule, we deliberately do not provide the convenience methods for performing simulations and force the user to create the \verb"Simulation" objects herself. 
The reason for this decision is twofold. 
Firstly, the convenience functions for simulations would have to have a lot of parameters, in order to be as flexible as the explicit creation of the simulator objects. 
This is a non-issue for the approximation objects, whose heavy-lifting function takes zero parameters, so all of the complexity is in the constructor parameters only.
Secondly, due to the way we perform numerical evaluations, subsequent uses of the same \verb"Simulation" object are much cheaper than the very first use of it which has to precompile the numeric routines. 
Shorthand function would not provide a way to exploit this caching of computation, as it would force to create a new simulation object, even if it could be reused, therefore we did not code that workflow.

Examples on how to use these classes are provided in \autoref{sec:example-simulation}.

\subsubsection{{\tt means.inference} -- parameter inference methods}
\subsubsection{{\tt means.io} -- reading and writing of \means-specific objects}
\subsubsection{{\tt means.util} -- utility functions used within the package}
\subsubsection{{\tt means.tests} -- tests for the software}
\todo{This mostly should be a link to testing section}
\subsubsection{{\tt means.examples} -- example models used in the documentation}
\subsubsection{{\tt means.pipes} -- convenience functions for data pipelines}
\todo{This should mostly be a link to pipelines section}

\subsection{Testing}
\subsection{Pipelines}
\todo{Let's use this section to explain how we generated this report.}

