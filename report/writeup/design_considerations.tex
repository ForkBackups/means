\section{Approach to Software Design}
\label{sec:design_considerations}

Our software development has been heavily inspired by the Agile methodology\cite{_manifesto_????}.
Agile methodology has been established as an alternative to the traditional waterfall method which views requirements gathering, software design and implementation as a linear process.
In contrast to this traditional view, agile methodology centres around iterative approach to software development, and aims to minimise the cost caused by problems along the way.

The key principles of this methodology have been neatly summarised by Kelly Waters\cite{_what_????}:

\blockquote{
    [The] characteristics that are common to all agile methods, and the things that I think make agile fundamentally different to a more traditional waterfall approach to software development [\ldots] are:
    
    \begin{enumerate}
        \item Active user involvement is imperative
        \item The team must be empowered to make decisions
        \item Requirements evolve but the timescale is fixed
        \item Capture requirements at a high level; lightweight \& visual
        \item Develop small, incremental releases and iterate
        \item Focus on frequent delivery of products
        \item Complete each feature before moving on to the next
        \item Apply the 80/20 rule
        \item Testing is integrated throughout the project lifecycle -â€“ test early and often
        \item A collaborative \& cooperative approach between all stakeholders is essential
    \end{enumerate}
}

While these may sound very vague requirements, they capture the recommendations based on years of trial-and-error on software development in the industry, and generally act as good guidelines to follow for projects with short deadlines.

\subsection{Requirements Gathering}
\label{sec:requirements-gathering}
One of the key parts of any project is the requirements gathering process -- no amount of skill or hard work will help if the work is misdirected.
Naturally, we took great interest in getting these requirements right.

Capturing requirements at ``\emph{at a high level}" was important for this project since the temptation to discuss about low-level implementation before gathering requirements was great.
These discussions are often constraining the requirements with the implementation details, while ideally we wanted the requirements to influence the implementation decisions, and not the other way around.

The easiest way to keep the requirements in a high level was to limit the discussion to a set of scenarios of usage.
These scenarios are commonly referred to \emph{user stories} and usually follow the template: ``As (some user type, \eg{} \emph{a scientist)} I would like to (some action, \eg{} \emph{perform these calculations}) because (optionally, the reason for this use case)".
The first part of this relatively free-form gives a view of the potential user base of the software.
For instance researchers and undergraduate students may have different requierements.
The second part, provides a set of important use cases for the problem.
Finally, the third part assesses the relative importance of each of the use cases.

We were able to write down four representative user stories, listed below:
\begin{enumerate}

    \item \emph{As a researcher studying biological systems, I would like to perform a variety of simulations, with different parameter sets, and compare the results of these simulations.}

    \item \emph{As a researcher studying the approximation method, I would like to be able to compare the changes to the approximation quality, when the approximation parameters (\ie{} the closure method) are changed.}

    \item \emph{As a researcher studying biological systems, I would like to be able to infer the set of parameters of the model given the data, but this idea is still not very popular in my field and I am not sure if it is feasible.}

    \item \emph{As a systems biologist, I am used to dealing with multiple data formats, and I am computer-savvy enough to adapt to any format implemented by the software, as long as it is reasonably simple.}
\end{enumerate}

The pattern emerging from these use cases was that our project software should be designed to be an intermediate step in some larger data analysis process, and should not try to limit the remaining steps of the process.
For instance, the \emph{comparison} process in the first two user stories could not be clarified in more detail, because the techniques are employed there in a very \emph{ad-hoc} fashion.

Similarly, the fourth user story indicates that the origin of the input data and its format are undefined.
Therefore the software needed be general enough to support multiple input formats, or have a simple enough format allowing researchers to convert their data to.

This observation encouraged us to move away from the traditional command-line application model for this software\cite{babtie_moment_2013}.
In our opinion, flexibility of command-line applications comes at the expense of simplicity -- we have all seen really flexible applications that have a list of parameters that is too long for a newcomer to comprehend.
Similarly, command-line applications create an artificial gap between the data processing and data analysis step, \ie{} writing and reading file output, which can be reduced if the application was returning the data in the format that can readily be used to investigate the data.

In order to satisfy these requirements, we designed \means{} package with flexibility and interactivity in mind. 
The flexibility requirement impacted certain decisions made while structuring the package, which are further described in \autoref{sec:package}.
Interactivity of the package has been ensured by coding specific behaviours for the IPython interactive python environment \cite{perez_ipython:_2007}.

\subsection{Package Structure}
\label{sec:package}

In order to make the code easier to maintain and manage, we have divided \means{} package into nine different smaller submodules, each responsible for a different part of the system.
We have paid particular attention in making all the necessary features importable using the pythonic shorthand \verb"from means import *", 
as to ensure it is simple enough to use \means{} in interactive environments where it might be uncomfortable to deal with modularity explicitly.

The nine submodules in \means{} ands their respective role are listed below:
\begin{enumerate}
    \item \verb"means.core" -- the core functionality, such as the definitions of key classes
    \item \verb"means.approximation" -- implementations of the \gls{lna} and \gls{mea} approximation methods.
    
    \item \verb"means.simulation" -- implementation of various \gls{ode} solving algorithms.
    \item \verb"means.inference" -- implementation of parameter inference methods
    \item \verb"means.io" -- implementation of input and output routines
    \item \verb"means.util" -- utility functions used across the modules, internal to the package
    \item \verb"means.tests" -- various tests for the package functionality, hidden from the user by default
    \item \verb"means.examples" -- definitions of example models that are used throughout tutorials, needs to explicitly be imported by the user.
    \item \verb"means.pipes" -- optional module providing wrappers for pipeline support
\end{enumerate}

Each submodule is described in detail below.

\subsubsection{{\tt means.core} -- core classes used within all other packages}
% If you are editing this, please also edit src/means/core/__init__.py docstring
% to keep the two consistent
This package stores the common classes that are used within all of the other means sub-packages.

The module exposes the descriptor classes, such as \verb"Descriptor",
\verb"VarianceTerm", \verb"Moment" and
\verb"ODETermBase" that are used to describe the types of trajectories generated,
as well as certain terms in the ODE equations.

Similarly, both the \verb"StochasticProblem" and
\verb"ODEProblem" classes that are used in stochastic and deterministic simulations respectively are exposed by this module.

Finally, the \verb"Model" class, which provides a standard interface to describe a biological model, and can be thought to be the center of the whole package, is also implemented here.

\subsubsection{{\tt means.approximation} -- approximation methods for biological models}

The approximation submodule implements \gls{lna} and \gls{mea} methods for biological systems encoded by the previously mentioned \verb"Model" objects (see \autoref{sec:making-a-model} for a detailed description on how this encoding is done).

The approximation package is itself split into two sub-packages, one for each of the approximation methods mentioned.
This keeps specific routines in separate modules, thus avoiding \gls{lna} and \gls{mea} implementations namespace polutiong each other.
This also provides a clear path for future developers to implement new approximation methods -- simply creating another sub-package.

Despite being in different packages, both approximation methods inherit from the same base class, intuitively named \verb"ApproximationBaseClass".
This base class defines the common interface all approximation methods; namely, the dependancy on \verb"Model" and public \verb"run()" method that implements the mathematics and returns a \verb"ODEProblem" object that can be simulated.
This common interface is required for any pipeline using the approximation methods to be method-agnostic.
In other words, we want the pipeline to be able to perform the approximation in exactly the same way, regardless of whether \gls{lna} or \gls{mea} was used. 
It is easy to see how this design pattern increases the code maintainability and compatibility as well: not only the future maintainer will have a clear template to follow for implementing an approximation method, but it will also be immediately compatible with all the packages using the approximations.

The advantages described above apply mostly for software using all or part of the \means{} package to perform designated tasks\sauliustodo{QG: not v clear}.
The necessity of creating approximation objects and running their \verb"run()" function in order obtain the \gls{ode}s soon becomes a burden in the interactive environments.
In order to overcome this, we also provide shorthand functions \verb"mea_approximation" and \verb"lna_approximation", the use of which is demonstrated in \autoref{sec:approximation-example}.

\subsubsection{{\tt means.simulation} -- stochastic and deterministic simulation routines}

The next submodule in the list provides the routines for performing deterministic and stochastic simulations.
These simulations are performed via \verb"Simulation" and \verb"SSASimulation" classes defined in this sub-module.

Unlike the approximation package, the simulation routines could not share the same interface, as they have different inputs -- a stochastic simulation needs \verb"StochasticProblem" that provides the stoichiometry matrix as well as hazard functions to simulate stochastically, while the deterministic simulation routine requires an \verb"ODEProblem" object that is essentially a collection of ordinary differential equations.
Nevertheless, the classes still share the same interface, providing a \verb"simulate_system()" method which in turn generates a set of \verb"Trajectory" objects, to keep them consistent.

Also unlike the approximation submodule, we deliberately do not provide the convenience methods for performing simulations and force the user to create the \verb"Simulation" objects herself. 
Firstly, the convenience functions for simulations would have to have a lot of parameters, in order to be as flexible as the explicit creation of the simulator objects. 
This is a non-issue for the approximation objects, whose heavy-lifting function takes zero parameters, so all of the complexity is in the constructor parameters only.
Secondly, due to the way we perform numerical evaluations, subsequent uses of the same \verb"Simulation" object are much cheaper than the very first use of it which has to precompile the numeric routines. 
Shorthand function would not provide a way to exploit this caching of computation, as it would force to create a new simulation object, even if it could be reused, therefore we did not code that workflow.

Examples on how to use both classes are provided in \autoref{sec:example-simulation}.

\subsubsection{{\tt means.inference} -- parameter inference methods}
% Change src/means/inference/__init__.py if you are modifying this
This part of the package provides utilities for parameter inference.
Parameter inference will try to find the set of parameters which
produces trajectories with minimal distance to the observed trajectories.

Different distance functions are implemented (such as functions minimising the sum of squares error
or functions based on parametric likelihood), but it is also possible to use custom distance functions.

The package provides support for both inference from a single starting point (\verb"Inference")
or inference from random starting points (\verb"InferenceWithRestarts").

Some basic inference result plotting functionality is also provided by the package, see the documentation for
\verb"InferenceResult" class for more information on this. \todo{Link to documentation/provide it in examples section}

\subsubsection{{\tt means.io} -- reading and writing of \means-specific objects}
% change src/means/io/__init__.py if you are changing this
The module implements common methods to serialise and deserialise \means objects.
Namely the module provides functions \verb`means.io.dump` and  \verb`means.io.load` that would
serialise and deserialise the said objects into YAML format\cite{_official_????}.
These serialised representations can be written to or read from files with the help of
\verb`means.io.to_file` and \verb`means.io.from_file` functions.
% this is not in __init__.py, but is required for report
The YAML format has been chosen due to it's ability to balance human and machine readability. 
Some examples of the YAML serialised objects can be seen in the \todo{write IO tutorial and link here}.

%this is in __init__.py again
For the user's convenience, the said methods are also attached to all serialisable objects,
e.g. \verb`means.core.Model.from_file()` method would allow the user to read a \verb`means.core.Model` object from file directly.

%also in __init__.py
We do not provide any convenience functions for binary serialisation of the object, because \verb`pickle` package,
which is in the default distribution of Python, has no problems of performing these tasks on \means objects. 
We recommend using \verb`pickle`, rather than \verb`means.io` whenever fast serialisation is preferred to human
readability.

% still in __init__.py
Finally, this module also provides support for the input from SBML files\cite{hucka_systems_2003}.
If the \verb`libsbml` is installed in the user's system and has the appropriate python bindings, the function \verb`means.io.read_sbml` can be used to parse the files in SBML format
to \verb`means.core.Model` objects. See example usage in \todo{Link to some tutorial}.

\subsubsection{{\tt means.util} -- utility functions used within the package}
% change src/means/util/__init__.py if you're changing this
This package contains helper functions that did not fit into any other packages.
These functions include helper functions for common operations when dealing with \verb`sympy`,
as well as functions that help with memoisation of CPU intensive function results.

These functions are designed to be package-specific.
The users of the software are generally discouraged to use any of these functions.

\subsubsection{{\tt means.tests} -- ensuring the working implementation}
The tests package contains, rather unsurprisingly, the tests for our implementation.
Our approach to testing is described in \autoref{sec:testing}

\subsubsection{{\tt means.examples} -- example models used in the documentation}
The examples package contain a set of toy models used in the documentation.
The models included with the package are:
\begin{itemize}
    \item a sample Michaelis-Menten kinetics model as in \cite{ale_general_2013};
    \item Dimerisation model, also in \cite{ale_general_2013};
    \item P53 model, described in \cite{ale_general_2013} and in \autoref{fig:p53} of this document;
    \item HES1 model, based on the description in \cite{ale_general_2013};
    \item A textbook example of Lotka-Volterra dynamics \cite{billard_lotka-volterra_1977}.
\end{itemize}

These example models are designed to help us to illustrate important concepts of the package in the documentation, but are otherwise independent from the package, and therefore need to be imported separately.
The user of the software is nevertheless encouraged to experiment with these toy models.

\subsubsection{{\tt means.pipes} -- convenience functions for data pipelines}
The \emph{pipes} submodel provides the convenience functions for pipeline tasks using \verb"luigi" package\cite{_luigi_????}.

The pipeline support is covered in detail in \autoref{sec:pipelines}.

\subsection{Testing}
\label{sec:testing}

``Test early and often" has now become one of the the catch-phrases associated with \emph{agile} software development (see \autoref{sec:design_considerations}). 
This catchy sentence gives a twofold advice: integrate testing as early as possible to your development lifecycle, and perform as many tests as possible.

Starting testing early is of particular importance because the cost of discovering problems increases exponentially with effort put into the process.  
For instance, we were quite happy to allow some extra time fixing an unexpected problem with our code early on in the project, but we would be seriously stressed if some riddle popped up as we are writing this report, due to the deadline approaching. 
Similarly, it some scenarios such problem might just invalidate all the previous hard work done earlier on.

In other words, early testing allows to specify the requirements
    \footnote{We use the word \emph{requirements} in a slightly different sense in this section compared to the one in \autoref{sec:requirements-gathering}. 
    Here the term \emph{requirements} is used from the code perspective and is synonymous to \emph{specification}, \emph{contract}, \emph{interface} of the code, whereas the same term in the previous section refers to the overall requirements of the software, which may be captured by a combination of these code-specific requirements described here.} 
for the code early on and allow us to ensure they continue to be satisfied as we progress through the development lifecycle. 


We were in particularly good position to start testing early, due to the work done by the people working on this project previously. 
As a starting point, we started by the output from this project as the true behaviour of the system, encode this requirement in the code, and find an automatic way of checking that this is still the case. 
We call these tests \emph{regression} tests as they are designed to validate that we have not introduced any new faults to the system,
essentially establishing a baseline performance of it.
We chose to use all of the examples given in the previous year's group report as a good starting point of our regression tests.

In order to \emph{test often}, as suggested by the same slogan mentioned earlier, we fired up an continuous integration server using Jenkins platform\cite{_jenkins_????}. The main purpose of this continuous integration server was to run our whole test suite after each commit, and ensure to notify us if there are problems with it. 
This way we can eliminate the human factor and ensure that the tests are really run often and completely and therefore ensure the continuous compliance with the specification.

As the software evolved, the regression tests were slowly phased out into unit tests for particular parts of the software. The main difference between the two types of the tests is that the regression tests test a large part of the system for preservation of a certain function, while unit tests focus on some small part of the system and explicitly tests it's behaviour independently. Given a large enough coverage of unit tests, we can be reasonably certain that the overall system also works fine, and thus eliminate the regression tests (which are often very slow to run). To illustrate this, better, picture a GC content counting algorithm for some DNA sequence. 
A regression test (sometimes also called functional test) would run the algorithm on whole DNA sequence and check that the overall output is, say, 60\%. 
Unit tests would only test a small subset of the system, for instance, it could check that given a particular window of DNA, the overall GC count is correctly computed in that window. Similarly, it could check that given a couple of counts obtained from such windows, they are averaged correctly and so on. 

It is possible to imagine that if combination of these unit tests are all fine, the functional test result should also be fine, so we are not losing any information by replacing these bulky tests with more fine-grained ones. It is also not hard to picture how this fine-graining could help spot problems with code faster, which is the main reason of these transitions.

\todo{Pick out the \% test coverage in jenkins and write about this}

\subsection{Pipelines}
\label{sec:pipelines}
\todo{Let's use this section to explain how we generated this report.}

