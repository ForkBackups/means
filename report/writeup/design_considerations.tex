\section{Approach to Software Design}
\label{sec:design_considerations}

Our software development has been heavily inspired by the Agile methodology\cite{_manifesto_????}.
Agile methodology has been established as an alternative to the traditional waterfall method which views requirements gathering, software design and implementation as a linear process.
In contrast to this traditional view, agile methodology centres around iterative approach to software development, and aims to minimise the cost caused by problems along the way.

The key principles of this methodology have been neatly summarised by Kelly Waters\cite{_what_????}:

\blockquote{
    [The] characteristics that are common to all agile methods, and the things that I think make agile fundamentally different to a more traditional waterfall approach to software development [\ldots] are:
    
    \begin{enumerate}
        \item Active user involvement is imperative
        \item The team must be empowered to make decisions
        \item Requirements evolve but the timescale is fixed
        \item Capture requirements at a high level; lightweight \& visual
        \item Develop small, incremental releases and iterate
        \item Focus on frequent delivery of products
        \item Complete each feature before moving on to the next
        \item Apply the 80/20 rule
        \item Testing is integrated throughout the project lifecycle -â€“ test early and often
        \item A collaborative \& cooperative approach between all stakeholders is essential
    \end{enumerate}
}

While these may sound very vague requirements, they capture the recommendations based on years of trial-and-error on software development in the industry, and generally act as good guidelines to follow for projects with short deadlines.

\subsection{Requirements Gathering}
One of the key parts of any project is the requirements gathering process -- no amount of skill or hard work will help if the work is misdirected.
Naturally, we took great interest in getting these requirements right.

Capturing requirements at ``\emph{at a high level}" was important for this project since the temptation to discuss about low-level implementation before gathering requirements was great.
These discussions are often constraining the requirements with the implementation details, while ideally we wanted the requirements to influence the implementation decisions, and not the other way around.

The easiest way to keep the requirements in a high level was to limit the discussion to a set of scenarios of usage.
These scenarios are commonly referred to \emph{user stories} and usually follow the template: ``As (some user type, \eg{} \emph{a scientist)} I would like to (some action, \eg{} \emph{perform these calculations}) because (optionally, the reason for this use case)".
The first part of this relatively free-form gives a view of the potential user base of the software.
For instance researchers and undergraduate students may have different requierements.
The second part, provides a set of important use cases for the problem.
Finally, the third part assesses the relative importance of each of the use cases.

We were able to write down four representative user stories, listed below:
\begin{enumerate}

    \item \emph{As a researcher studying biological systems, I would like to perform a variety of simulations, with different parameter sets, and compare the results of these simulations.}

    \item \emph{As a researcher studying the approximation method, I would like to be able to compare the changes to the approximation quality, when the approximation parameters (\ie{} the closure method) are changed.}

    \item \emph{As a researcher studying biological systems, I would like to be able to infer the set of parameters of the model given the data, but this idea is still not very popular in my field and I am not sure if it is feasible.}

    \item \emph{As a systems biologist, I have to deal with multiple data formats on case by case basis, and would like to support them all or have a simple format that could be easily writeable, because I am computer-savvy enough to write a conversion script.}
    \todo{QG: I struggle with this last story.}
\end{enumerate}

The pattern emerging from these use cases was that our project software should not be designed to act as a standalone piece of software, \todo{QG: second part of sentence ambiguous}and merely be an intermediate step in some larger data analysis step.
For instance, the \emph{comparison} process in the first two user stories could not be clarified in more detail, because the techniques employed in that step are chosen in a very \emph{ad-hoc} fashion.

Similarly, the fourth user story indicates that the origin of the input data and its format are undefined.
Therefore the software needed be general enough to support multiple input formats, or have a simple enough format allowing researchers to convert their data to.
This was not unsurprising, given the exploratory nature of the method,\todo{QG: not clear} of course, yet not less important to achieve.

This observation encouraged us to move away from the traditional command-line application model for this software\todo{cite last year report}.
In our opinion, flexibility of command-line applications comes at the expense of simplicity -- we have all seen really flexible applications that have a list of parameters that is too long for a newcomer to comprehend.
Similarly, command-line applications create an artificial gap between the data processing and data analysis step, \ie{} writing and reading file output, which can be reduced if the application was returning the data in the format that can readily be used to investigate the data.

\todo{"Instead, we ... " We need to bi rigth to the point : we made a python package!!, it is called MEANS! :D}
We believe that the best way we can achieve both flexibility and minimise the gap between the data processing and analysis with the help of package-like structure and interactive python environments, such as IPython notebook\todo{Cite ipython notebook}.
We therefore have spent our development efforts on designing an intuitive modular system for the package, and ensuring integration with this environment.
This structure is further described in \autoref{sec:package}.


\subsection{Package Structure}
\label{sec:package}

In order to make the code easier to maintain and manage, we have divided \means{} package into nine different smaller submodules, each responsible for a different part of the system.
We have paid particular attention in making all the necessary features importable using the pythonic shorthand \verb"from means import *", 
as to ensure it is simple enough to use \means{} in interactive environments where it might be uncomfortable to deal with modularity explicitly.

The nine submodules in \means{} ands their respective role are listed below:
\begin{enumerate}
    \item \verb"means.core" -- the core functionality, such as the definitions of key classes
    \item \verb"means.approximation" -- implementations of the \gls{lna} and \gls{mea} approximation methods.
    
    \item \verb"means.simulation" -- implementation of various \gls{ode} solving algorithms.
    \item \verb"means.inference" -- implementation of parameter inference methods
    \item \verb"means.io" -- implementation of input and output routines
    \item \verb"means.util" -- utility functions used across the modules, internal to the package
    \item \verb"means.tests" -- various tests for the package functionality, hidden from the user by default
    \item \verb"means.examples" -- definitions of example models that are used throughout tutorials, needs to explicitly be imported by the user.
    \item \verb"means.pipes" -- optional module providing wrappers for pipeline support
\end{enumerate}

Each submodule is described in detail below.

\subsubsection{{\tt means.core} -- core classes used within all other packages}
% If you are editing this, please also edit src/means/core/__init__.py docstring
% to keep the two consistent
This package stores the common classes that are used within all of the other means sub-packages.

The module exposes the descriptor classes, such as \verb"Descriptor",
\verb"VarianceTerm", \verb"Moment" and
\verb"ODETermBase" that are used to describe the types of trajectories generated,
as well as certain terms in the ODE equations.

Similarly, both the \verb"StochasticProblem" and
\verb"ODEProblem" classes that are used in stochastic and deterministic simulations respectively are exposed by this module.

Finally, the \verb"Model" class, which provides a standard interface to describe a biological model, and can be thought to be the center of the whole package, is also implemented here.

\subsubsection{{\tt means.approximation} -- approximation methods for biological models}

The approximation submodule implements \gls{lna} and \gls{mea} methods for biological systems encoded by the previously mentioned \verb"Model" objects (see \autoref{sec:making-a-model} for a detailed description on how this encoding is done).

The approximation package is itself split into two sub-packages, one for each of the approximation methods mentioned.
This keeps specific routines in separate modules, thus avoiding \gls{lna} and \gls{mea} implementations namespace polutiong each other.
This also provides a clear path for future developers to implement new approximation methods -- simply creating another sub-package.

Despite being in different packages, both approximation methods inherit from the same base class, intuitively named \verb"ApproximationBaseClass".
This base class defines the common interface all approximation methods; namely, the dependancy on \verb"Model" and public \verb"run()" method that implements the mathematics and returns a \verb"ODEProblem" object that can be simulated.
This common interface is required for any pipeline using the approximation methods to be method-agnostic.
In other words, we want the pipeline to be able to perform the approximation in exactly the same way, regardless of whether \gls{lna} or \gls{mea} was used. 
It is easy to see how this design pattern increases the code maintainability and compatibility as well: not only the future maintainer will have a clear template to follow for implementing an approximation method, but it will also be immediately compatible with all the packages using the approximations.

The advantages described above apply mostly for software using all or part of the \means{} package to perform designated tasks\todo{QG: not v clear}.
The necessity of creating approximation objects and running their \verb"run()" function in order obtain the \gls{ode}s soon becomes a burden in the interactive environments.
In order to overcome this, we also provide shorthand functions \verb"mea_approximation" and \verb"lna_approximation", the use of which is demonstrated in \autoref{sec:approximation-example}.

\subsubsection{{\tt means.simulation} -- stochastic and deterministic simulation routines}

The next submodule in the list provides the routines for performing deterministic and stochastic simulations.
These simulations are performed via \verb"Simulation" and \verb"SSASimulation" classes defined in this sub-module.

Unlike the approximation package, the simulation routines could not share the same interface, as they have different inputs -- a stochastic simulation needs \verb"StochasticProblem" that provides the stoichiometry matrix as well as hazard functions to simulate stochastically, while the deterministic simulation routine requires an \verb"ODEProblem" object that is essentially a collection of ordinary differential equations.
Nevertheless, the classes still share the same interface, providing a \verb"simulate_system()" method which in turn generates a set of \verb"Trajectory" objects, to keep them consistent.

Also unlike the approximation submodule, we deliberately do not provide the convenience methods for performing simulations and force the user to create the \verb"Simulation" objects herself. 
Firstly, the convenience functions for simulations would have to have a lot of parameters, in order to be as flexible as the explicit creation of the simulator objects. 
This is a non-issue for the approximation objects, whose heavy-lifting function takes zero parameters, so all of the complexity is in the constructor parameters only.
Secondly, due to the way we perform numerical evaluations, subsequent uses of the same \verb"Simulation" object are much cheaper than the very first use of it which has to precompile the numeric routines. 
Shorthand function would not provide a way to exploit this caching of computation, as it would force to create a new simulation object, even if it could be reused, therefore we did not code that workflow.

Examples on how to use both classes are provided in \autoref{sec:example-simulation}.

\subsubsection{{\tt means.inference} -- parameter inference methods}
% Change src/means/inference/__init__.py if you are modifying this
This part of the package provides utilities for parameter inference.
Parameter inference will try to find the set of parameters which
produces trajectories with minimal distance to the observed trajectories.

Different distance functions are implemented (such as functions minimising the sum of squares error
or functions based on parametric likelihood), but it is also possible to use custom distance functions.

The package provides support for both inference from a single starting point (\verb"Inference")
or inference from random starting points (\verb"InferenceWithRestarts").

Some basic inference result plotting functionality is also provided by the package, see the documentation for
\verb"InferenceResult" class for more information on this. \todo{Link to documentation/provide it in examples section}

\subsubsection{{\tt means.io} -- reading and writing of \means-specific objects}
% change src/means/io/__init__.py if you are changing this
The module implements common methods to serialise and deserialise \means objects.
Namely the module provides functions \verb`means.io.dump` and  \verb`means.io.load` that would
serialise and deserialise the said objects into \todo{figure out how I want to cite this}YAML format.
These serialised representations can be written to or read from files with the help of
\verb`means.io.to_file` and \verb`means.io.from_file` functions.
% this is not in __init__.py, but is required for report
The YAML format has been chosen due to it's ability to balance human and machine readability. 
Some examples of the YAML serialised objects can be seen in the \todo{write IO tutorial and link here}.

%this is in __init__.py again
For the user's convenience, the said methods are also attached to all serialisable objects,
e.g. \verb`means.core.Model.from_file()` method would allow the user to read a \verb`means.core.Model` object from file directly.

%also in __init__.py
We do not provide any convenience functions for binary serialisation of the object, because \verb`pickle` package,
which is in the default distribution of Python, has no problems of performing these tasks on \means objects. 
We recommend using \verb`pickle`, rather than \verb`means.io` whenever fast serialisation is preferred to human
readability.

% still in __init__.py
Finally, this module also provides support for the input from SBML files.
If the \verb`libsbml` is installed in the user's system and has the appropriate python bindings, the function \verb`means.io.read_sbml` can be used to parse the files in SBML format
to \verb`means.core.Model` objects. See example usage in \todo{Link to some tutorial}.

\subsubsection{{\tt means.util} -- utility functions used within the package}
% change src/means/util/__init__.py if you're changing this
This package contains helper functions that did not fit into any other packages.
These functions include helper functions for common operations when dealing with \verb`sympy`,
as well as functions that help with memoisation of CPU intensive function results.

These functions are designed to be package-specific.
The users of the software are generally discouraged to use any of these functions.

\subsubsection{{\tt means.tests} -- ensuring the working implementation}
The tests package contains, rather unsurprisingly, the tests for our implementation.
Our approach to testing is described in \autoref{sec:testing}

\subsubsection{{\tt means.examples} -- example models used in the documentation}
The examples package contain a set of toy models used in the documentation.
The models included with the package are:
\begin{itemize}
    \item a sample Michaelis-Menten kinetics model as in \cite{ale_general_2013};
    \item Dimerisation model, also in \cite{ale_general_2013};
    \item P53 model, described in \cite{ale_general_2013} and in \autoref{fig:p53} of this document;
    \item HES1 model, based on the description in \cite{ale_general_2013} \todo{Was it in ale?};
    \item A textbook example of Lotka-Volterra dynamics \todo{QG: cite the original LV book/paper}.
\end{itemize}

These example models are designed to help us to illustrate important concepts of the package in the documentation, but are otherwise independent from the package, and therefore need to be imported separately.
The user of the software is nevertheless encouraged to experiment with these toy models.

\subsubsection{{\tt means.pipes} -- convenience functions for data pipelines}
The \emph{pipes} submodel provides the convenience functions for pipeline tasks using \verb"luigi" package. \todo{Cite luigi somehwere, figure out how}.

The pipeline support is covered in detail in \autoref{sec:pipelines}.

\subsection{Testing}
\label{sec:testing}
\todo{Testing testing testing OY!}
\subsection{Pipelines}
\label{sec:pipelines}
\todo{Let's use this section to explain how we generated this report.}

