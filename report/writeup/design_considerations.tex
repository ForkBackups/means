\section{Approach to Software Design}
\label{sec:design_considerations}

Our software development has been heavily inspired by the Agile methodology\cite{_manifesto_????}. 
Agile methodology has been established as an alternative to the traditional waterfall method which views requirements gathering, software design and implementation as a linear process. 
In contrast to this traditional view, agile methodology centres around iterative approach to software development, and aims to minimise the cost caused by problems along the way.

The key principles of this methodology have been neatly summarised in a blog post by Kelly Waters\cite{_what_????}:

\blockquote{
    [The] characteristics that are common to all agile methods, and the things that I think make agile fundamentally different to a more traditional waterfall approach to software development [..] are:
    
    \begin{enumerate}
        \item Active user involvement is imperative 
        \item The team must be empowered to make decisions 
        \item Requirements evolve but the timescale is fixed 
        \item Capture requirements at a high level; lightweight \& visual 
        \item Develop small, incremental releases and iterate 
        \item Focus on frequent delivery of products 
        \item Complete each feature before moving on to the next 
        \item Apply the 80/20 rule 
        \item Testing is integrated throughout the project lifecycle -â€“ test early and often 
        \item A collaborative \& cooperative approach between all stakeholders is essential 
    \end{enumerate}
}

While these may sound very vague requirements, they capture the recommendations based on years of trial-and-error on software development in the industry, and generally act as good guidelines to follow for projects with short deadlines.

\subsection{Requirements Gathering}
One of the key parts of any project is the requirements gathering process -- no amount of skill or hard work will help if the work is misdirected.
Naturally, we took great interest of getting these requirements right.

As per the blogpost cited above, it is suggested to ``capture requirements \emph{at a high level}". 
``High level" is emphasised here to warn about the temptation to get into the discussions about low-level implementation details in the requirements gathering stage. 
These discussions are often in a risk to constrain the requirements with the implementation details, while ideally we would like the requirements to influence the implementation decisions, and not the other way around.

The easiest way to keep the requirements in a high level is to limit the discussion to a set of scenarios of how the users would like to use the software. These scenarios are commonly referred to \emph{user stories} and usually follow the template ``As (some user type, e.g. \emph{a scientist)} I would like to (some action, e.g. \emph{perform these calculations}) because (optionally, the reason for this use case)".
The first part of this relatively free-form stories allows us to get a view of the potential user base of the software, e.g. whether it is researchers who are going to be the software, or maybe it is mostly undergraduate students, each requiring different approach to the problem. The second part of these stories allows us to get a set of important use cases for the problem, while the third part allow us to judge the relative importance of each of the use cases.

We were able to write down four user stories, listed below:
\begin{enumerate}

    \item \emph{As a researcher studying biological systems, I would like to perform a variety of simulations, with different parameter sets, and compare the results of these simulations.}

    \item \emph{As a researcher studying the approximation method, I would like to be able to compare the changes to the approximation quality, when the approximation parameters (i.e. the closure method) are changed.}

    \item \emph{As a researcher studying biological systems, I would like to be able to infer the set of parameters of the model given the data, but this idea is still not very popular in the field and I am not sure if its feasible.}

    \item \emph{As a systems biologist, I have to deal with multiple data formats on case by case basis, and would like to support them all or have a simple format that could be easily writable, because I am computer-savvy enough to write a conversion script.}
\end{enumerate}

The pattern emerging from these use cases is that our project software should not be designed to act as a standalone piece of software, and merely be an intermediate step in some larger data analysis step. For instance, the \emph{comparison} process in the first two user stories could not be clarified in more detail, because the techniques employed in that step are chosen in a very \emph{ad-hoc} fashion. 

Similarly, the fourth user story basically tells us that it is not clear where the input for the model will come from and which format it would be therefore the software needs to be able to either be general enough to work with them all or have a simple enough format for the researcher to do the conversion herself. 
This is not at all surprising, given the exploratory nature of the method, of course, yet not less important to achieve. 

It was this observation that prompted us to move away from the traditional command-line application model for this software. 
In our opinion, flexibility of command line applications comes with the expense of simplicity -- we have all seen really flexible applications that have a list of parameters that is too long for a newcomer to comprehend. Similarly, command line applications create an artificial gap between the data processing and data analysis step, i.e. writing and reading file output, which can be reduced if the application was returning the data in the format that can readily be used to investigate the data.

We believe that the best way we can achieve both flexibility and minimise the gap between the data processing and analysis with the help of package-like structure and interactive python environments, such as IPython notebook\todo{Cite ipython notebook}.
We therefore have spent our development efforts on designing an intuitive modular system for the package, and ensuring integration with this environment. This structure is further described in \autoref{sec:package}.

\subsection{Package Structure}
\label{sec:package}

In order to make the code easier to maintain and manage, we have chosen to structure the main \means  package into nine different smaller submodules, each responsible for a different part of the system. We have taken a great care to ensure that all the necessary features are still importable using the pythonic shorthand \verb"from means import *", as to ensure it is easy enough to use in interactive environments where it might be uncomfortable to deal with modularity explicitly.

The nine submodules mentioned earlier and their functions are listed below:
\begin{enumerate}
    \item \verb"means.core" -- the core functionality, such as the definitions of key classes
    \item \verb"means.approximation" -- implementations of the \gls{lna} and \gls{mea} approximation methods.
    
    \item \verb"means.simulation" -- implementation of various \gls{ode} solving algorithms.
    \item \verb"means.inference" -- implementation of parameter inference methods
    \item \verb"means.io" -- implementation of input and output routines
    \item \verb"means.util" -- utility functions used across the modules, internal to the package
    \item \verb"means.tests" -- various tests for the package functionality, hidden from the user by default
    \item \verb"means.examples" -- definitions of example models that are used throughout tutorials, needs to explicitly be imported by the user.
    \item \verb"means.pipes" -- optional module providing wrappers for pipeline support
\end{enumerate}

Each of these submodules are described in detail below.

\subsubsection{{\tt means.core} -- core classes used within all other packages}
% If you are editing this, please also edit src/means/core/__init__.py docstring
% to keep the two consistent
This package stores the common classes that are used within all of the other means sub-packages.

The module exposes the descriptor classes, such as \verb"Descriptor",
\verb"VarianceTerm", \verb"Moment" and
\verb"ODETermBase" that are used to describe the types of trajectories generated,
as well as certain terms in the ODE equations.

Similarly, both the \verb"StochasticProblem" and
\verb"ODEProblem" classes that are used in stochastic and deterministic simulations respectively are exposed by this module.

Finally, the \verb"Model" class, which provides a standard interface to describe a biological model, and can be thought to be the center of the whole package, is also implemented here.