This document summarises the major contributions of Saulius Lukauskas
to the development of \means{} package.
This document has been written as a succinct supplement to the group report, and therefore it contains numerous back-references to sections of the larger document.

\section{Introduction}

\section{Contributions}
\subsection{Software Development Process}
\todo{Quote science code manifesto}
In the modern day where computers are becoming the driving force of innovation and progress the whole world is facing the need to either adjust to this computerisation or risk to be left behind by others.
Naturally, the need to use and computer software is becoming increasingly important for scientific communities in every major field of science. The extent of this has already made some knowledge of computer programming a requirement skill to succeed.

While knowledge of computer programming is deemed critical, the necessity to study software-engineering is still being debated. 
The common argument for the lack of need to study software engineering, claims that people often care only about the end result, and not how it was obtained or how easy-to-read code is. 
I find this claim ironic as the same argument is actually the main reason why the software-engineering methodologies emerged in the first place. 
For instance, two of the main principles of the \emph{agile} methodology\cite{_manifesto_????} claim treat the ``working software as the primary measure of success" and claim that ``simplicity -- the art of maximising of work \emph{not} done -- is essential"\cite{paulk_agile_2002}, clearly in par with the same argument described earlier. 
In fact \emph{agile} methodology is viewed as an alternative to the approaches that aim to create this idealistic, properly structured and well-documented software, such as the traditional \emph{waterfall} model.

It was my personal goal, and I believe my biggest contribution, to share my experience in software engineering in order to incorporate the \emph{agile} methodology into our development cycle. 
In this project I aimed to not only introduce my peers to concepts that might have not been known to them previously, but to also push these concepts to the full potential and immediately validate their usefulness by showing how they allow reaching the desired goals faster. 

For instance, not only we were using \verb"git"\cite{git} for our version control, but we were also using appropriate branching workflow. 
This allowed us to ensure our work that is still relatively experimental does not impact other people working on different things. 
In turn, it had immediate result of providing the courage to experiment with the code, courage without which the performance improvements described in the group report's section \autoref*{GR:performance} may have never been made.

Another factor that contributed to the ability to change the key parts of the system without the fear of breaking something was the early incorporation of automated tests to the system. 
This created a way to validate the correctness of our software, and have a way to pinpoint a failure, minutes after the change that introduced it. 
In turn, it allowed us to spend less time testing our software, but trust it to be correct more.
In turn, this increases our confidence in the validity of the observations made (as described in the \autoref*{GR:results} of group report). Our approach to software testing is summarised in the main report's \autoref*{GR:sec:testing}.

To help ourselves \emph{increase the amount of work not done}, we set up a continuous integration server. 
This server, implemented by the \emph{Jenkins CI} platform\cite{_jenkins_????}, has responsibly been running every single test we have after every single push to the central code repository we made. At the moment of writing, the server has run 507 of these tests.
We assumed that if the code does not work on \emph{Jenkins}, as we called it, it does not work at all, regardless of whether the code works on the author's machine. 
Such code could then be retracted, fixed and pushed again to the main repository, at which point it may or may not pass the tests. 
This allowed us to reduce the number of times we have to deal with files forgotten to be added to the central repository, or tests forgotten to be run before committing due to human error.

Similarly, \emph{Jenkins} enabled us to run code in different environments. For instance, we had one environment where where we always kept the libraries updated to the latest version. 
It was because of this special environment that we spotted an incompatibility with the latest version of \verb"Assimulo"\cite{andersson_christian_assimulo:_????} -- the package providing ODE solver implementations in \py{} -- minutes after that version became public. We were then able to prove that this incompatibility was a problem with the \verb"Assimulo" package, not ours. 
We then worked together with Cristian Andersson (one of the developers of \verb"Assimulo") to it get fixed. 
In the sense, this continuous integration environment has contributed to two projects -- \verb"Assimulo" and \means{} -- as if not for it, said bug in the library might still be present today.

Finally, I explained in detail the advanced concepts of Object-Oriented Programming, such as inheritance, encapsulation, static methods to my peers so they could be applied in our project.
I attempted to show how these concepts make the code both easier to maintain, and easier to write in the first place, hopefully allowing the peers to transfer these skills to their other projects as well.
In the end we have succeeded at creating a highly modular codebase, which we believe should be easy to maintain for the next person lucky enough to work on this project. The full summary of our code base is available in the \autoref*{GR:sec:package} of the group report.

Besides the programming-related best practices of software engineering, we also implemented certain project management aspects suggested by \emph{agile} methodologies. 
For instance, we structured our work into weekly iterations. We were planning the things we need to do for the week, and reviewing the things we done at the end of it. 
We have structured our iterations to end just before the weekly meetings with supervisors, so we could also immediately get the feedback.
This allowed to keep the project on a clear track and going the right way.


In the end, I believe that the software-engineering approaches I tried to introduce to the team were good both as an educational exercise and a way to be more efficient, when it comes to software development.

\subsection{Significant Code Contributions}

The previous section covered my contributions to the software engineering in general.
In this section I list my most significant contributions to the codebase.

\subsubsection{Implementation of Simulation Routines}

One of the first things I worked on in this projection was an attempt to simplify the mechanism for solving ODE equations implemented the previous year's version of the code. 
The original mechanism would split the process into three parts: the \python{} code would be transcribed of into C, which would then be compiled to a library interacting with the \verb"CVODE" solver\cite{hindmarsh_sundials_2005}, which then would be run directly from python.
We understood that the compilation to C was necessary to speed up the computations, but this rather \emph{ad-hoc} process of transcribing \py{} to C seemed to be error prone and unnatural in general.  

We found \verb"Assimulo" -- a package already mentioned once before in this report -- to be a suitable replacement for the wrappers around \verb"CVODE" that would allow us to easily extend the functionality as well. 
I have implemented the change to use this package, attempting it to be a transparent one, that would not change anything in other areas of code. Similarly, I have I made \means{} exploit the full functionality provided by the said package, adding interfaces to perform sensitivity analysis and ability to choose a different solver. 
This new interface is illustrated in detail in the \autoref*{GR:sec:simulation} of the main report.

In order to keep the performance of the simulation as fast as the original implementation, we had to find a way to evaluate numeric expressions efficiently, as this has proven to be the bottleneck. 
QG and I have investigated alternative ways of performing numeric evaluations in \verb"sympy" -- the python package for symbolic mathematics. We benchmarked the \verb"lambdify" method, that turns the numeric expression into a \verb"lambda" function and the use of  \verb"autowrap" module which compiles the numeric expressions to C.
We did not test the third option -- Theano\cite{bergstra_theano:_2010} -- available in the latest version of \verb"sympy", because the college environment did not support it yet\footnote{We eventually updated the college environment to run the latest version of the package, however we did not get back to this benchmark due to the time constraints.}.

\begin{table}
    \centering
    \begin{tabular}{l|l}
    Method & Runtime \\
    \hline
    Default & 628 microseconds \\
    \verb"lambdify" & 10.7 microseconds \\
    \verb"autowrap" & 2.89 microseconds \\
    Theano & not measured \\
    \end{tabular}
    \caption{Results of the three out of four numeric evaluation methods available in {\tt sympy}. 
    The results were recorded using the {\tt \%timeit} 
    function provided by IPython\cite{perez_ipython:_2007} interactive environment. 
    This function performs as many evaluations of the code it can fit in 2 seconds and returns the best runtime of all the repetitions.
    The evaluations were performed on the two right-hand-side equations resulting from approximating the dimerisation model up to the moment order of two.
    It is clear from the data that{\tt autowrap} method is a clear winner. 
    Theano performance was not tested as it was not supported in the college environment at the time we performed this test.}
    \label{tab:numeric-runtimes}
\end{table}

The benchmarking results from the experiment are summarised in \autoref{tab:numeric-runtimes}. 
We can see that \verb"autowrap" is clear winner in the benchmark and therefore was chosen to be implemented in our package.
This has proven to be an efficient-enough solution for most of the day-to-day needs. 
In the future, however, it would be interesting to benchmark the Theano performance, whose inhering GPU support could speed up the evaluations even more.
In particularly, I believe it would help to reduce the exponential slope of runtime arising from the exponential increase of the number of equations as the maximal moment order is increased (see \autoref{GR:fig:solver-runtimes} in the group report) to make the simulations feasible for even higher orders. 

\subsubsection{Implementation of Inference Routines}

Similarly to the simulation methods described above, I have also worked on restructuring the parameter inference to make it easier to use from interactive environment. 
Namely we aimed to unify the interfaces to parameter distance methods regardless of the distance function used, and allow the user to specify the requirements succinctly.
I believe that both of these requirements were met, and the new interface for parameter inference is demonstrated in \autoref*{GR:sec:parameter_inference} of the group report.

Additionally, the unification of the parameter inference interfaces enabled our users to design their own distance functions, as demonstrated in \autoref*{GR:sec:distance_measures} of the group report.
Finally, I have worked with SF on the additional plotting functionality for the parameter inference results, such as the contour plot of the distance landscape, illustrated in 
\autoref*{GR:sec:investigating_distance_landscape} of the group report.

\subsubsection{Pipeline Support}

\subsection{Investigation of MEA and Solver Performance}

\section{Conclusions}

