\newpage{}
\pagenumbering{arabic}

\section{Introduction}

For our group project, we wrote a \py{} package for Moment Expansion Approximation, Inference and Simulation (\means).
Our group report explains our motivations, results and findings in detail.
Briefly, we provided a tool to model dynamic systems based on \acrlong{mea}\cite{ale_general_2013}.
Our implementation is well documented, maintainable, extensible and user friendly.
It also performs considerably faster than previous implementations.

In the report herein, I describe some of my major contributions to the project.
First of all, I explain \emph{why} and \emph{how} the implementation of \acrlong{mea} was eventually completely rewritten.
Then, I describe how parametric moment closure was translated from \mat{} prototype to be included in a unified framework in \means.
As explained in the group report, a considerable part of my work was also to improve the performance of \gls{mea}.
Here, I provide more details about the how this was realised.  
I then briefly explain the rationale behind integrating \acrlong{ssa} simulations in the package.
Finally, I investigate whether our conclusions apply to other systems such as \emph{Hes1}.

\section{Restructuring \acrlong{mea} Implementation}
As we attempted to read the \py{} implementation of \acrshort{mea} written by last year students\cite{babtie_moment_2013},
we were deterred by several points.
First of all, the code was often unnecessarily intricate and hard to understand.
Then, it was not organised in a very modular fashion.
Finally, established \py{} coding and documentation conventions\cite{_pep_????} were not respected.
These issues were limiting in so far as they reduced readability and made it hard to comprehend and improve the code.
We believe it would not have been possible, for us, to implement new features, such as parametric moment closure,
without having largely restructured the code in a first place.

My main individual contribution was to reforge the core implementation of \acrshort{mea} (as opposed to working on simulation and inference).
Symbolic computation were performed with \sympy{}\cite{sympy_development_team_sympy:_2014},
because this library is efficient, well documented and was already used by the exiting python implementation\cite{babtie_moment_2013}.
 
 
\subsection{Simplifying the Code}
We started by recording results from the original implementation and wrote ``regression tests''.
In this way, every time the code was amended, tests were run to ensure validity of mathematical results was preserved.

%A major issue with the code was readability; in many cases, it was necessary to run the code in order to understand its role.
%In this section, I illustrate, by three representative examples, how the original code was rewritten.

Readability of the code was a major issue for us.
In many cases, it was necessary to run the code in order to understand its role.
In this section, I illustrate, by three representative examples, how the original code was rewritten.

In the original code, the first step of the implementation of \acrshort{mea} was to generate a list of vectors of integers describing moments orders.
This list contained vectors($c$) for possible combinations of $d$ integers such as:\\
\[\mathbf{c} =(n_1, \dots, n_d) \in \mathbf{N^d}\]
satisfying the condition:\\
\[\sum_{i=1}^{d} n_i < o\]
up to moments of order $o$.\\

This was achieved through a very intricate recursive function which was extremely difficult to read and modify.
Instead, we used the \texttt{product()} function, in the standard module \texttt{itertools},
to generate all $o^d$ combinations of $c$ vectors of length  $d$, in one single line of code.
It was then trivial to filter the resulting list in order to keep only vectors with a sum lower or equal to the highest order of moments $o$.

Another striking example of complexity of the original code what the calculation of partial derivatives.
In order to compute the result of eq.~7 of \gls{mea}\cite{ale_general_2013} it is necessary to calculate\\
\[\frac{\partial{}^\mathbf{n} a_l(x)}{\partial{}\mathbf{x^n}}\]
where,\\
\[\partial{}^\mathbf{n} = \partial{}^\mathbf{n_1+\dots{}+n_d}\]
\[\partial{} \mathbf{x^n} = \partial{} x^n_1 \dots{} \partial x^n_d\]

In the original code, the property:
\begin{equation}
\label{eq:partial}
\frac{\partial{} ^ 2 f(x,y)}{\partial x \partial y} =
\frac{\partial{} \frac{\partial{} f(x,y)}{\partial x}}{\partial y} =
\frac{\partial{} \frac{\partial{} f(x,y)}{\partial{} y}}{\partial{} x}
\end{equation}
was used to pre-compute partial derivatives starting from low order and using the result to derive higher order.
All derivative were stored in a three dimensional hierarchical construct (named \texttt{`damat'}).
Further in the code, it was necessary to perform  tedious index calculations to
retrieve partial derivatives for each moment order combination.
This was both hard to understand the structure of \texttt{damat} and to retrieve values from it.
In addition, this type of unusual structure was deprecated in the following versions of \sympy{}.
Investigating the documentation of \sympy, it appeared that it was in fact possible to directly compute partial derivatives
with respect to arbitrary number and order of variables.
For instance, \texttt{sympy.diff(y,x1,x1,x2)} would return $\frac{\partial^3 y}{\partial x_1^2 \partial x_2}$.
This was later optimised (see section~\ref{sec:optimisation}).

A last example is the use of symbols describing moments (raw and central). In the original code,
every time a symbol describing a moment was needed, it was necessary to create a string from a vector of orders with the correct naming convention.
For example, a raw mixed moment could be described by a string such as \verb"x_123".
``Hard-coding'' the names and naming conventions rendered it very difficult and error-prone to change the notations for symbols.
For instance, we judged better to use \verb"x_a_b" instead of \verb"x_ab" as symbol for a raw moment of order ($a$ and $b$)
because the latter notation would be ambiguous if ever moments were computed for orders higher than ten (\eg{} \verb"x_111" could mean either \verb"x_11_1"  or \verb"x_1_11").
Instead, in \means, two lists of `Moment' objects are created instead of two list of vectors of orders (one for raw moments and one for central moments).
Each \texttt{Moment} contains a variable for `symbol' and a vector of orders.
Therefore, future developers can work with Moment objects without having to know the naming convention.
This approach also renders later substitution of symbols by integers or other symbols unnecessary.
In contrast, in the original code, central moments were encoded as \verb"ym_ab" and then
converted to \verb"yxN" (where \verb"N"  is a positive integer),
and $0^{th}$ order raw moments had to be converted to $1$.
Our approach allows to allocate the final symbol (or value) once for all at the start, which greatly simplifies the code.

\subsection{Modularity}

The original implementation was monolithic.
There were a few functions, which were essentially executed once.
However, there were many repeated processes in the code.
For instance, two Taylor expansions used the same principles,
so it was often possible to write functions that several parts of the code could use.
This obviously shortened and organised the code, but also made it much simpler to profile and improve the it.
 
In addition, no classes were used.
Object-Oriented Programming is a powerful concept allowing to organise and reuse code very efficiently.
In the original code, there were a large function to perform \gls{lna}, and another one to perform \gls{mea}.
However, some of the tasks performed by both methods rely on the same principle: using a model as an input and generating a set of \glspl{ode} as an output.
Therefore, it was very advantageous to make a base class for all approximation method, from which both \gls{lna} and \gls{mea} derived.
In addition, object oriented programming allowed us to define custom data structures with their own methods.
For instance custom \texttt{ODEProblems} and \texttt{Moments} objects were very helpful to represent mathematical concepts.

\subsection{Using \py{} Standards}
The original code used indexed for loops (like in \texttt{C}).
However, in \py, ``foreach'' loops are  standard and are much more efficient and readable.
In addition, advanced syntactic feature of \py{} such as list comprehensions  and mapping,
which are very concise and efficient, where not used.
For instance, in the original code, filtering a vector for values higher then six would done like:

\begin{minted}{python}
vector = [1, 5, 7, 8, 3]

filtered_vector = []
for i in range(0, len(vector)):
    if vector[i] > 6:
        filtered_vector.append(vector[i])
\end{minted}

Using list comprehension, this can be done in one line:

\begin{minted}{python}
filtered_vector = [v for v in vector if v > 6]
\end{minted}

This may look insignificant for such a simplistic example, but it becomes very hard to manage multiple 
nested loops (which were numerous) with the former version.
Moreover, it is more efficient to use iterators rather than indexing.

In \means, we took advantage of these features to improve the code.
In addition, the entire code was documented following established standards.
This allows, for instance, to automatically generate online documentation for developers and users.


%\cite{ale_general_2013}

\begin{table}[tbh]
\caption{\emph{Examples of variable name choices.}
Comparison of some  variable names between original \py{} code, our package (\means) and the reference publication.
}

\begin{center}   
    \begin{tabular}{ | l | l | r|}
    \hline
    \bf{Publication\cite{ale_general_2013}} & \bf{Original code} & \bf{\means}\\
    \hline
    \hline
    $\frac{d_{\mu}}{dt}$ & \verb"M" & \verb"dmu_over_dt"\\
    \hline
    $\mathbf{n\choose{k}}$ & \verb"f2" & \verb"n_choose_k"\\
    \hline
    $ (-1)^\mathbf{{n-k}}$ & \verb"f3" & \verb"minus_one_pow_n_minus_k"\\
    \hline
    $\mathbf{n_1    , n_2, ..., n_d}$ & \verb"counter" & \verb"n_counter"\\
    \hline
    $\mathbf{k_1, k_2, ..., k_d}$ & \verb"mcounter" & \verb"k_counter"\\
    \hline
    \end{tabular}
\end{center}
\label{tab:varnames}
\end{table}

Another important, but often neglected, aspect of readability is the choice of variable names.
Having explicit self explanatory variable names often reduces cognitive load.
Modern code editor allow auto-completion of variable names, so it is not limiting to use long names.
We aimed to name variables according to the symbols in the original publication\cite{ale_general_2013}.
Table~\ref{tab:varnames} represents a few examples of how we chose variable names.


\section{Parametric Moment Closure Implementation}
Implementation of moment closures using parametric distributions was done in collaboration
with Sisi Fan because prototypes were available in \mat{}\cite{lakatos_preparation_2014}. She had experience using \mat{},
whilst I had more practice with \py{}.
Here, I only describe my decisions about the implementation in \py{}.

All closure methods essentially replace higher order (\gls{maxord}~+~1) raw moment symbols with expressions depending on lower order moments (or constants).
For parametric closure, raw moments are expressed in terms of lower order moments.
In addition, for univariate distributions, mixed raw moments are always set to $0$.
In the \mat{} prototype, all closures were implemented independently without any explicit common structure.
In addition, the default (scalar) closure was implicit and implemented very differently to other closures.

In \means, we have unified all closure methods in a common base class (\texttt{ClosureBase}).
The base class implements all methods that all closures will use (\eg{} substitutions, setting cross terms to $0$).
Then, only the method to compute raw moments has to be specialised in subclasses.
This general framework facilitated optimisation and will greatly simplify the task of researchers willing
to implement closure methods for other distributions.

\section{Optimisation of Symbolic Computations}

\label{sec:optimisation}

In this section, I detail how optimisations of symbolic calculation were performed (fig.~\ref*{GR:fig:mea_speed} in the group report).
As a reminder, the number of equations generated by \gls{mea} for a system with $s$ species and up to \gls{maxord} of $o$
is defined by:
\begin{equation}
    \text{Number of equations} = {{s + o} \choose {s}} - 1
    \label{eq:number_of_equations}
\end{equation}

As a consequence, the symbolic computations become extremely costly if the number of species or \gls{maxord} increase.
Therefore, it was extremely important to optimise performance of \gls{mea} in order to make it available for models with more species.

\subsection{Profiling}

Optimisation was realised using \py{} profiling tools.
This provides valuable information such as how many times a function is called, and how long, in total,
the interpreter spent ``in'' this function (which is yet another reason use modularity and write functions).
Figure~\ref{fig:profiled} represents the result of such a profile obtained during development.

\begin{figure}[tbh]
\begin{centering}
\includegraphics[width=0.83\textwidth{}]{profile.pdf}
\caption{\emph{Simplified profile of MEA using means.}
 Each box indicates a function.
 Coloured arrows indicate a ``caller $ \rightarrow $ callee'' relation.
 The first row in each box is the name of the function (prefixed with the module name and the line number).
 The second row indicates the total time spend calling this function (directly or indirectly).
 The last row is the total number of time a function has been called.
 Black arrows indicate \sympy{} functions with large footprints that were explicitly called by our package.
 This example profile suggests that symbol substitution could be an issue (a).
 The function \texttt{sympy.solve()} seems to have also a large footprint (b).
 Finally differentiation (\texttt{diff}) is an expansive function (c).
 Altogether, the interpreter spent approximatively $60\%$
 of the runtime in these three functions.
 Note that this specific profile has been pruned to facilitate representation.
}
\label{fig:profiled}
\end{centering}
\end{figure}


\subsection{Improving Performance Iteratively}
In this section, I detail the nature of the optimisations performed in section~\ref*{GR:sec:optimising_mea} of the group report.
The relevant figure is reproduced as figure ~\ref{fig:mea_speed} of the document herein.

\begin{figure}[tbh]
\begin{centering}
\includegraphics[width=0.95\textwidth{}]{mea_speed.pdf}
\caption{\emph{Cumulative performance improvement of symbolic
calculations resulting from optimisation}.
The processing time for computing log-normal closure on \pft{} model with different \gls{maxord}s were measured for original Matlab implementation (a) and different optimisations (b$-$f).
In a first place, the calls to \texttt{sympy.simplify()} were removed (b).
Then, \texttt{sympy.xreplace()} was used instead of \texttt{sympy.substitute()} (c).
Computing a rectangular matrix containing expressions for all moment was more efficient than generating a square matrix to later remove unneeded expressions (d).
Implementing a simplified equation solver instead of using \texttt{sympy.solve()} also resulted in a significant speed-up (e).
Finally, caching (memorisation) \texttt{sympy.diff()} allowed even better performance.
The time complexity appears exponential ($O(2^k)$, where $k$ is the maximal moments order) in every case,
Nine replicates were performed on the same CPU.
For optimisation c$-$f, values corresponding to \gls{maxord} moments lower than two were removed because of
the inherent inaccuracy in measuring very short durations.}
\label{fig:mea_speed}
\end{centering}
\end{figure}


The first optimisation step involved removing the ``expression simplification'' heuristic.
In the original code (from both publication\cite{ale_general_2013} and last year's MSc project\cite{babtie_moment_2013}),
the right-hand-side equations were simplified in order to produce shorter text file results.
However, this was slow and did not benefit subsequent simulations and inference.
For large expressions, simplification also had an large memory footprint and was likely to fail.
This removing simplification routines significantly improved the scalability of the method (see fig.~\ref{fig:mea_speed}b).

The next bottleneck was the choice of substitution functions.
As a part of \gls{mea}, it is necessary to replace raw moment symbols by expressions depending on central moments.
Performing substitution can be done using the \texttt{substitute()} function from \sympy, but this is designed to substitute expressions by other expressions.
In most cases, we only had to substitute atomic symbols by expression.
For this purpose, the \texttt{xreplace()} function was a much more appropriate alternative which resulted in a better scalability (see fig.~\ref{fig:mea_speed}c).

In the original implementation, a matrix of central moment expression of size
\[(n-s) \times (n-s + 1)\]
where,
$n$ is the number of equations (see eq.~\ref{eq:number_of_equations}) and
$s$ is the number of species,
was directly generated for scalar closure.
However, for parametric closure, a matrix of size $(n_2-s) \times (n_2-s + 1)$,
where,
\[n_2={{s+o \mathbf{+1}} \choose {s}} -1\]
was generated.
The $n_2 - n$ rows corresponding to higher-order moments were unused and eventually deleted.
In contrast, our implementation generates a $(n-s) \times (n_2-s + 1)$ matrix regardless of the closure method.
In addition to making the code more readable, consistent and flexible, this improved overall performance (fig.~\ref{fig:mea_speed}d)
for cases where closure is applied whilst keeping the default closure computation fast.

Another simple way to improve computation time was to remove calls to the function \texttt{solve()} which was only used in straightforward cases
(\eg{} solving: $x + 2y = z$ for $x$).
It was therefore much more efficient (fig.~\ref{fig:mea_speed}e) to use trivial arithmetic to find solutions.

Finally, partial derivation of expression over several variables and order is extensively performed during the approximation.
Generally, these type of differentiations can be simplified several differentiation of first order (see eq.~\ref{eq:partial})
One advantage, is that, when needing to calculate two derivatives such as: $\frac{\partial{} ^ 2 f(x,y)}{\partial{} x \partial{} y}$ and $\frac{\partial{} ^ 2 f(x,y)}{\partial{} x^2}$,
one can precompute $\frac{\partial{} f(x,y)}{\partial{} x}$ and use it for both calculation.
In our implementation, we have use a procedure known as \emph{memoization} which, briefly, permits to store the results of a function call in an associative array.
Then, the next time this function is called with the same arguments, it will return the stored results instead of recomputing it.
This also resulted in an overall performance improvement (fig.~\ref{fig:mea_speed}f).

Interestingly, the slopes between, optimisations a ($0.95$) and c ($0.58$), and b ($0.62$) and c were significantly different ($p-value <10^{-15}$ and $p-value = 3 \times 10^{-4}$, respectively;
t-test on the slopes of the linear regression). This indicates that optimisation c ``scales'' better that b and a.
No significant difference was found between the slopes of the subsequent optimisations (c$-$f).
However, the intercepts were significantly smaller between each consecutive optimisations after c) ($p-value < 10^{-6}$ for all; t-test on the intercepts of the linear regression).

Compared to the original \mat{} implementation, these improvement are very significant. We estimate than, for computing \gls{mea} with log-normal closure up to \gls{maxord}
of 8 in the \pft{} system, our implementation is approximately 5000 times faster.

\section{Implementation of \acrlong{ssa}}
Finally, when we started to investigate the accuracy of \gls{mea}, 
we realised that it was necessary to compare simulation results to \gls{ssa} average\cite{gillespie_general_1976}.
Considering the number of parameter sets we needed, it was not practical to generate average trajectories using external programs.
We therefore implemented our own version of the exact, direct, algorithm.
The algorithm is quite simple, so it was quick to provide a basic implementation.
The most important for our research was to integrate stochastic simulations consistently with the rest of the package.
For instance, it needed to return the same type of objects as simulation based on approximation.
In addition, it was necessary that \gls{ssa} simulations could return empirical moments as collections of trajectories,
which allows to compare mean, variance, covariance, and so on, between simulations.
If, in the future, a faster implementation of \gls{ssa} is needed, it should be straightforward to optimise this code.



\section{Multivariate Closure and Hes1 System}
The \pft{} model was very interesting to study. However, we could not draw general conclusions on \gls{mea} from this model only.
For this reason, we also performed preliminary work on the \emph{Hes1} model\cite{ale_general_2013}.
Figure~\ref{fig:hes1} shows the effect of different closure methods and \gls{maxord} on the accuracy of the approximation.
This is similar to figure~\ref*{GR:fig:max_order_and_closure_on_distance_summary} in the group report (which was made for \pft).
\begin{figure}[tbh]
\begin{centering}
\includegraphics[width=0.95\textwidth{}]{FigureHes1Summary-pdf-7.pdf}
\caption{\emph{Effect of different closure methods and \gls{maxord} on simulation accuracy}.
The \emph{Hes1} system was modelled using \gls{mea} with five types of closure and for \gls{maxord} up to seven.
Resulting trajectories were all compared to an average of 5000 \gls{ssa} simulations using sum of square distance.
Distance is in log scale.
Log-normal closures are not represented, because they generated  unstable trajectories with very large distances.
}
\label{fig:hes1}
\end{centering}
\end{figure}

Interestingly, log-normal closure was not capable of generating stable trajectories (most distances were higher than $10^{10}$, or the solver failed).  
In contrast with \pft{} model, multivariate normal closure performed systematically better than the univariate counterpart.
Also, scalar is generally better than univariate normal closure.
Finally, as for \pft, it is not obvious, with this set of parameter, that higher \gls{maxord} results in more accurate approximation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
This project was extremely collaborative, and many aspect of our research were carried conjointly with my two peers.
Here, I have described which part of the projects I have lead.
\gls{mea} with parametric moment closures was the novelty and the originality of this research.
It was therefore paramount for us to implement it as well as possible.
It was also our goal to provide a package that allowed alternative simulation and approximations methods such as \gls{ssa}.
Although these two points had immediate benefits for us, our implementation has been driven by the needs of the community in a first instance.
At this stage, we believe \means{} is ready to be made available for the scientific public.

